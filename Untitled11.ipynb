{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled11.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMMqxaixwxtmacTWTfBgi6n",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saadahm/Saadahm1/blob/main/Untitled11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AW318pfJCeH"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import tensorflow as tf"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DecEcoBSJKMY"
      },
      "source": [
        "is_ipython = 'inline'  in matplotlib.get_backend()\n",
        "if is_ipython: from IPython import display"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHxqORSSJMsK"
      },
      "source": [
        "class DQN(nn.Module):\n",
        "  def __init__(self,input_size):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(in_features=input_size,out_features=64)\n",
        "    self.fc2 = nn.Linear(in_features=64,out_features=8)\n",
        "    self.out = nn.Linear(in_features=8,out_features=3)\n",
        "  def forward(self,t):\n",
        "    t = t.flatten(start_dim=1)\n",
        "    t = F.relu(self.fc1(t))\n",
        "    t = F.relu(self.fc2(t))\n",
        "    t = self.out(t)\n",
        "    return t  \n"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_2BkAmBJMyE"
      },
      "source": [
        "Experience = namedtuple('Experience',('state','action','next_state','reward'))"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ToEWVTAJTp-"
      },
      "source": [
        "class ReplayMemory():\n",
        "  def __init__(self,capacity):\n",
        "    self.capacity = capacity\n",
        "    self.memory = []\n",
        "    self.push_count =0\n",
        "  def push(self,experience):\n",
        "    if len(self.memory)<self.capacity:\n",
        "      self.memory.append(experience)\n",
        "    else:\n",
        "      self.memory[self.push_count%self.capacity] = experience\n",
        "      self.push_count+=1\n",
        "  def sample(self,batch_size):\n",
        "    return random.sample(self.memory,batch_size)\n",
        "  def can_provide_sample(self,batch_size):\n",
        "   # print(len(self.memory))\n",
        "    return len(self.memory)>=batch_size  "
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBc5_NwbJVyy"
      },
      "source": [
        "class EpsilonGreedyStrategy():\n",
        "  def __init__(self,start,end,decay):\n",
        "    self.start = start\n",
        "    self.end = end\n",
        "    self.decay = decay\n",
        "  def get_exploration_rate(self,current_step):\n",
        "    return self.end + (self.start-self.end)*math.exp(-1.*current_step*self.decay)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1omflFaiJYZ5"
      },
      "source": [
        "class Agent():\n",
        "  def __init__(self,strategy,num_actions,device):\n",
        "    self.current_step = 0 \n",
        "    self.strategy = strategy\n",
        "    self.num_actions = num_actions\n",
        "    self.device = device \n",
        "  def select_action(self,state,policy_net):\n",
        "    rate = strategy.get_exploration_rate(self.current_step)\n",
        "    self.current_step +=1\n",
        "    if rate> random.random():\n",
        "      action =  random.randrange(self.num_actions)  # explore\n",
        "      return torch.tensor([action]).to(device)\n",
        "    else:\n",
        "      with torch.no_grad():\n",
        "        #print(state,\"f1\")\n",
        "        return policy_net(state).argmax(dim=1).to(device) # exploit"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNzLIvk9JdO5"
      },
      "source": [
        "\n",
        "def formatPrice(n):\n",
        "    return(\"-Rs.\" if n<0 else \"Rs.\")+\"{0:.2f}\".format(abs(n))\n",
        "def getStockDataVec():\n",
        "    vec = []\n",
        "    lines = open(\"/content/NFLX.csv\",\"r\").read().splitlines()\n",
        "    for line in lines[1:2267]:\n",
        "        #print(line)\n",
        "        #print(float(line.split(\",\")[4]))\n",
        "        vec.append(float(line.split(\",\")[4]))\n",
        "        #print(vec)\n",
        "    return vec \n",
        "def sigmoid(x):\n",
        "    return 1/(1+math.exp(-x))\n",
        "\n",
        "def getState(data, t, n):\n",
        "    d = t - n + 1\n",
        "    block = data[d:t + 1] if d >= 0 else -d * [data[0]] + data[0:t + 1] # pad with t0\n",
        "    res = []\n",
        "    for i in range(n - 1):\n",
        "        res.append(sigmoid(block[i + 1] - block[i]))\n",
        "    return np.array([res])\n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AG6JqREL8Nz"
      },
      "source": [
        "def plot(values, moving_avg_period):\n",
        "    plt.figure(2)\n",
        "    plt.clf()        \n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(values)\n",
        "    plt.plot(get_moving_average(moving_avg_period, values))\n",
        "    plt.pause(0.001)\n",
        "    if is_ipython: display.clear_output(wait=True)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcsKUn4yMAdi"
      },
      "source": [
        "def get_moving_average(period, values):\n",
        "    values = torch.tensor(values, dtype=torch.float)\n",
        "    if len(values) >= period:\n",
        "        moving_avg = values.unfold(dimension=0, size=period, step=1).mean(dim=1).flatten(start_dim=0)\n",
        "        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n",
        "        return moving_avg.numpy()\n",
        "    else:\n",
        "        moving_avg = torch.zeros(len(values))\n",
        "        return moving_avg.numpy()"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwpO_L14MIYa"
      },
      "source": [
        "def extract_tensors(experiences):\n",
        "    # Convert batch of Experiences to Experience of batches\n",
        "    batch = Experience(*zip(*experiences))\n",
        "\n",
        "    t1 = torch.cat(batch.state)\n",
        "   # print(t1)\n",
        "    t2 = torch.cat(batch.action)\n",
        "    t3 = torch.cat(batch.reward)\n",
        "    t4 = torch.cat(batch.next_state)\n",
        "\n",
        "    return (t1,t2,t3,t4)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXRDQoTIWxi-"
      },
      "source": [
        "class QValues():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    def get_current(policy_net, states, actions):\n",
        "      return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
        "    def get_next(target_net, next_states):\n",
        "      final_state_locations = next_states.flatten(start_dim=1).max(dim=1)[0].eq(0).type(torch.bool)\n",
        "      non_final_state_locations = (final_state_locations == False)\n",
        "      non_final_states = next_states[non_final_state_locations]\n",
        "      batch_size = next_states.shape[0]\n",
        "      values = torch.zeros(batch_size)#.to(QValues.device)\n",
        "      values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n",
        "      return values  "
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P71-1ogEMN-G"
      },
      "source": [
        "batch_size = 256\n",
        "gamma = 0.999\n",
        "eps_start = 1\n",
        "eps_end = 0.01\n",
        "eps_decay = 0.001\n",
        "target_update  = 10\n",
        "memory_size = 100000\n",
        "lr = 0.01\n",
        "num_episodes = 50\n"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r67wezQmMP0h"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#em = CartPoleEnvManager(device)\n",
        "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
        "agent = Agent(strategy, 3, device)\n",
        "memory = ReplayMemory(memory_size)\n",
        "vec = getStockDataVec()\n",
        "k=len(vec)\n",
        "#print(k)"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aagr4VUHMSH9"
      },
      "source": [
        "\n",
        "policy_net = DQN(64).to(device)\n",
        "target_net = DQN(64).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n",
        "window_size = 64"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR-sXNPFNTpL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cddffff-b90b-4cf2-cb5f-3f8d7a65482b"
      },
      "source": [
        "episode_durations = []\n",
        "for episode in range(num_episodes):\n",
        "    print(\"Episode \" + str(episode) + \"/\" + str(num_episodes))\n",
        "    state = torch.tensor(getState(vec, 0, window_size + 1))\n",
        "    total_profit = 0\n",
        "    invent = []\n",
        "    #state = getState(vec,)\n",
        "    max_transaction = 20 \n",
        "    total_money = 10000\n",
        "    c_s_h = 0\n",
        "    c_t_c =0\n",
        "    for t in range(k-1):\n",
        "      action = agent.select_action(state.float(), policy_net.float())\n",
        "      #print(1,action)\n",
        "      next_state =  torch.tensor(getState(vec, t+1, window_size + 1))\n",
        "      reward =0\n",
        "      if action == 0 and c_t_c < max_transaction and total_money>0:\n",
        "       # print(\"f2\")\n",
        "        x=total_money/(max_transaction-c_t_c) \n",
        "        total_money = total_money - x\n",
        "        c_t_c +=1\n",
        "        \n",
        "        x= x/vec[t]\n",
        "        c_s_h += x\n",
        "        a=[]\n",
        "        a.append(x)\n",
        "        a.append(vec[t])\n",
        "        invent.append(a)\n",
        "         #print(\"Buy: \" + formatPrice(data[t]))\n",
        "      elif action ==1 and len(invent)>0:\n",
        "        #print(\"f1\")\n",
        "        b_p = invent.pop(0)\n",
        "        reward = vec[t]*b_p[0]-b_p[0]*b_p[1]\n",
        "        total_money += vec[t]*b_p[0]\n",
        "        total_profit += reward \n",
        "        #print(reward)\n",
        "        c_s_h = c_s_h - b_p[0]\n",
        "        c_t_c =0\n",
        "      elif len(invent)>0:\n",
        "        b_p = invent[0]\n",
        "        #print(len(invent))\n",
        "        reward = -vec[t]*b_p[0]+b_p[0]*b_p[1]  \n",
        "      #action = torch.tensor(action)  \n",
        "     \n",
        "      #print(action.shape)\n",
        "      reward = torch.tensor([reward])\n",
        "      memory.push(Experience(state, action, next_state, reward))\n",
        "      state = next_state\n",
        "      if memory.can_provide_sample(batch_size):\n",
        "        experiences = memory.sample(batch_size)\n",
        "        states, actions, rewards, next_states = extract_tensors(experiences)\n",
        "        current_q_values = QValues.get_current(policy_net, states.float(), actions)\n",
        "        next_q_values = QValues.get_next(target_net, next_states.float())\n",
        "        target_q_values = (next_q_values * gamma) + rewards\n",
        "        loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "      done = True if t == k-2 else False  \n",
        "      if done:\n",
        "        # episode_durations.append(t)\n",
        "        # plot(episode_durations, 100)\n",
        "         break\n",
        "    print(total_profit)    \n",
        "    if episode % target_update == 0:\n",
        "      target_net.load_state_dict(policy_net.state_dict())"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 0/50\n",
            "4360.271179057909\n",
            "Episode 1/50\n",
            "4081.309903352995\n",
            "Episode 2/50\n",
            "799.240766423849\n",
            "Episode 3/50\n",
            "595.0456568484684\n",
            "Episode 4/50\n",
            "701.196581409802\n",
            "Episode 5/50\n",
            "391.9997544107586\n",
            "Episode 6/50\n",
            "-681.7254732168719\n",
            "Episode 7/50\n",
            "500.080575349234\n",
            "Episode 8/50\n",
            "576.191472085037\n",
            "Episode 9/50\n",
            "-216.41863333614697\n",
            "Episode 10/50\n",
            "705.6382777704497\n",
            "Episode 11/50\n",
            "11089.664284121385\n",
            "Episode 12/50\n",
            "854.3144666169414\n",
            "Episode 13/50\n",
            "46.427004110285\n",
            "Episode 14/50\n",
            "669.0182346815006\n",
            "Episode 15/50\n",
            "381.69813251453706\n",
            "Episode 16/50\n",
            "50.5809104990812\n",
            "Episode 17/50\n",
            "129.16472795718227\n",
            "Episode 18/50\n",
            "135.21259434449854\n",
            "Episode 19/50\n",
            "1469.7558362216341\n",
            "Episode 20/50\n",
            "-929.6272670393187\n",
            "Episode 21/50\n",
            "6008.370456324693\n",
            "Episode 22/50\n",
            "314.56948585170244\n",
            "Episode 23/50\n",
            "24.72108660519234\n",
            "Episode 24/50\n",
            "421.33709914878455\n",
            "Episode 25/50\n",
            "79.49642111492807\n",
            "Episode 26/50\n",
            "531.2216677584968\n",
            "Episode 27/50\n",
            "317.38442717862773\n",
            "Episode 28/50\n",
            "-61.94569265318927\n",
            "Episode 29/50\n",
            "-1767.9977880751157\n",
            "Episode 30/50\n",
            "-49.15285205182204\n",
            "Episode 31/50\n",
            "287.8532825468143\n",
            "Episode 32/50\n",
            "967.5174621142701\n",
            "Episode 33/50\n",
            "27.772172321936182\n",
            "Episode 34/50\n",
            "956.6738843366468\n",
            "Episode 35/50\n",
            "-689.3798780312961\n",
            "Episode 36/50\n",
            "1883.1154574836514\n",
            "Episode 37/50\n",
            "1067.5151951229159\n",
            "Episode 38/50\n",
            "79.4897441502959\n",
            "Episode 39/50\n",
            "142.88356659024515\n",
            "Episode 40/50\n",
            "346.6049946323331\n",
            "Episode 41/50\n",
            "2347.9801702114096\n",
            "Episode 42/50\n",
            "104.08752327343774\n",
            "Episode 43/50\n",
            "68.62230906458728\n",
            "Episode 44/50\n",
            "1005.8384041725806\n",
            "Episode 45/50\n",
            "800.014237756984\n",
            "Episode 46/50\n",
            "208.49052468874675\n",
            "Episode 47/50\n",
            "215.84687846247192\n",
            "Episode 48/50\n",
            "293.7087981680092\n",
            "Episode 49/50\n",
            "254.11281964227555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pToT-UfEO8pM",
        "outputId": "e90804cb-5fb2-458f-e6df-c1dba07ca371"
      },
      "source": [
        "print(total_profit)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "254.11281964227555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdXC5vgVTeNw",
        "outputId": "8ef666c2-94be-4f9f-fe3c-166b896ca323"
      },
      "source": [
        "max_transaction = 50\n",
        "total_money = 10000\n",
        "c_s_h = 0\n",
        "c_t_c =0\n",
        "print(k)\n",
        "for t in range(k-1):\n",
        "      action = agent.select_action(state.float(), policy_net.float())\n",
        "      #print(action)\n",
        "      next_state =  torch.tensor(getState(vec, t+1, window_size + 1))\n",
        "      reward =0\n",
        "      if action == 0 and c_t_c < max_transaction and total_money>0:\n",
        "       # print(\"f2\")\n",
        "        x=total_money/(max_transaction-c_t_c) \n",
        "        total_money = total_money - x\n",
        "        c_t_c +=1\n",
        "        \n",
        "        x= x/vec[t]\n",
        "        c_s_h += x\n",
        "        a=[]\n",
        "        a.append(x)\n",
        "        a.append(vec[t])\n",
        "        invent.append(a)\n",
        "        print(\"Buy: \" + formatPrice(vec[t]))\n",
        "      elif action ==1 and len(invent)>0:\n",
        "        #print(\"f1\")\n",
        "        b_p = invent.pop(0)\n",
        "        reward = vec[t]*b_p[0]-b_p[0]*b_p[1]\n",
        "        total_money += vec[t]*b_p[0]\n",
        "        total_profit += reward \n",
        "        print('profit :', total_profit)\n",
        "        #print(reward)\n",
        "        c_s_h = c_s_h - b_p[0]\n",
        "        c_t_c =0\n",
        "      elif action ==2 and len(invent)>0:\n",
        "        b_p = invent[0]\n",
        "        #print(len(invent))\n",
        "       # print('f1')\n",
        "        reward = vec[t]*b_p[0]-b_p[0]*b_p[1]  \n",
        "      #action = torch.tensor(action)  \n",
        "     \n",
        "      #print(action.shape)\n",
        "      reward = torch.tensor([reward])\n",
        "      memory.push(Experience(state, action, next_state, reward))\n",
        "      state = next_state\n",
        "      \n",
        "      done = True if t == k-2 else False  \n",
        "      if done:\n",
        "        #episode_durations.append(timestep)\n",
        "        #plot(episode_durations, 100)\n",
        "        break\n",
        "print(total_profit)\n",
        "print(total_money)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2265\n",
            "Buy: Rs.6.66\n",
            "profit : 252.56840522786115\n",
            "Buy: Rs.20.81\n",
            "profit : 253.22727793756584\n",
            "Buy: Rs.51.83\n",
            "profit : 253.38161077617303\n",
            "Buy: Rs.89.15\n",
            "profit : 253.04832584872517\n",
            "Buy: Rs.101.25\n",
            "profit : 249.76965465148174\n",
            "249.76965465148174\n",
            "9995.656835009206\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "Z3CRbHh4MWk4",
        "outputId": "c624b0d4-61bc-4119-bb46-9f4a451462a7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.9881, 0.3682, 0.9483,  ..., 0.9742, 0.2405, 0.1431],\n",
            "        [0.4583, 0.4679, 0.4169,  ..., 0.2392, 0.5495, 0.7600],\n",
            "        [0.3765, 0.4075, 0.4986,  ..., 0.6251, 0.1755, 0.3177],\n",
            "        ...,\n",
            "        [0.2159, 0.1809, 0.7389,  ..., 0.9870, 0.2369, 0.1256],\n",
            "        [0.3705, 0.9816, 0.7341,  ..., 0.4804, 0.0373, 0.7325],\n",
            "        [0.4375, 0.4886, 0.5004,  ..., 0.4871, 0.5367, 0.5449]],\n",
            "       dtype=torch.float64)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-52532305a49a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mcurrent_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQValues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_current\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mnext_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQValues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mtarget_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnext_q_values\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_q_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_q_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'QValues' has no attribute 'get_next'"
          ]
        }
      ]
    }
  ]
}